---
title: 'SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge Distillation
  in Magnetic Resonance Image Reconstruction'
abstract: Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration
  have shown remarkable success in providing high-quality reconstruction. However,
  as the number of cascades increases, the improvements in reconstruction tend to
  become marginal, indicating possible excess model capacity. Knowledge distillation
  (KD) is an emerging technique to compress these models, in which a trained deep
  teacher’ network is used to distill knowledge to a smaller student’ network, such
  that the student learns to mimic the behavior of the teacher. Most KD methods focus
  on effectively training the student with a pre-trained teacher that is unaware of
  the student model. We propose SFT-KD-Recon, a student-friendly teacher training
  approach along with the student as a prior step to KD to make the teacher aware
  of the student’s structure and capacity and enable aligning the teacher’s representations
  with the student. In SFT, the teacher is jointly trained with the unfolded branch
  configurations of the student blocks using three loss terms - teacher-reconstruction
  loss, student-reconstruction loss, and teacher-student imitation loss, followed
  by KD of the student. We perform extensive experiments for MRI acceleration in 4x
  and 5x under-sampling, on the brain and cardiac datasets on five KD methods using
  the proposed approach as a prior step. We consider the DC-CNN architecture and setup
  teacher as D5C5 (141765 parameters), and student as D3C5 (49285 parameters) denoting
  2.87:1 compression. Results show that (i) our approach consistently improves the
  KD methods with improved reconstruction performance and image quality, and (ii)
  the student distilled using our approach is competitive with the teacher, with the
  performance gap reduced from 0.53 dB to 0.03 dB.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: matcha24a
month: 0
tex_title: 'SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge Distillation
  in Magnetic Resonance Image Reconstruction'
firstpage: 1423
lastpage: 1440
page: 1423-1440
order: 1423
cycles: false
bibtex_author: Matcha, NagaGayathri and Ramanarayanan, Sriprabha and Fahim, Mohammad
  Al and S, Rahul G and Ram, Keerthi and Sivaprakasam, Mohanasankar
author:
- given: NagaGayathri
  family: Matcha
- given: Sriprabha
  family: Ramanarayanan
- given: Mohammad Al
  family: Fahim
- given: Rahul G
  family: S
- given: Keerthi
  family: Ram
- given: Mohanasankar
  family: Sivaprakasam
date: 2024-01-23
address:
container-title: Medical Imaging with Deep Learning
volume: '227'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 23
pdf: https://proceedings.mlr.press/v227/matcha24a/matcha24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
