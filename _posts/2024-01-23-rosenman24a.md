---
title: Pre-Training Transformers for Fingerprinting to Improve Stress Prediction in
  fMRI
abstract: We harness a Transformer-based model and a pre-training procedure for fingerprinting
  on fMRI data, to enhance the accuracy of stress predictions. Our model, called MetricFMRI,
  first optimizes a pixel-based reconstruction loss. In a second unsupervised training
  phase, a triplet loss is used to encourage fMRI sequences of the same subject to
  have closer representations, while sequences from different subjects are pushed
  away from each other. Finally, supervised learning is used for the target task,
  based on the learned representation. We evaluate the performance of our model and
  other alternatives and conclude that the triplet training for the fingerprinting
  task is key to the improved accuracy of our method for the task of stress prediction.
  To obtain insights regarding the learned model, gradient-based explainability techniques
  are used, indicating that sub-cortical brain regions that are known to play a central
  role in stress-related processes are highlighted by the model.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rosenman24a
month: 0
tex_title: Pre-Training Transformers for Fingerprinting to Improve Stress Prediction
  in fMRI
firstpage: 212
lastpage: 234
page: 212-234
order: 212
cycles: false
bibtex_author: Rosenman, Gony and Malkiel, Itzik and Greental, Ayam and Hendler, Talma
  and Wolf, Lior
author:
- given: Gony
  family: Rosenman
- given: Itzik
  family: Malkiel
- given: Ayam
  family: Greental
- given: Talma
  family: Hendler
- given: Lior
  family: Wolf
date: 2024-01-23
address:
container-title: Medical Imaging with Deep Learning
volume: '227'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 23
pdf: https://proceedings.mlr.press/v227/rosenman24a/rosenman24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
